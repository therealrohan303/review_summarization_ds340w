{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1vnFO2RU8jVT",
        "outputId": "3694f1ff-5c3f-41af-bfd5-69f4c3cd5d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets evaluate rouge-score sentencepiece torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXBnoRHJ8qHW",
        "outputId": "85284813-578c-4a74-ddc5-49888399e257"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data/AmaSum\n",
        "\n",
        "!unzip -q /content/drive/MyDrive/AmaSum/raw_min_10_max_100_revs.zip -d /content/data/AmaSum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5KkNjJHVDc5B",
        "outputId": "ad906769-2860-4e4e-f6a4-ddc26641671e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/data/AmaSum/min_10_max_100_revs_filt_complete/note.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yuanyuanlei-nlp/polarity_calibration_naacl_2024.git\n",
        "%cd polarity_calibration_naacl_2024"
      ],
      "metadata": {
        "id": "57cj22FVAomJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4b138ae5-f070-44ca-b623-a3811fd6312c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'polarity_calibration_naacl_2024' already exists and is not an empty directory.\n",
            "/content/polarity_calibration_naacl_2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib, os\n",
        "\n",
        "repo = root/\"polarity_calibration_naacl_2024\"\n",
        "amasum = root/\"data\"/\"AmaSum\"/\"min_10_max_100_revs_filt_complete\"\n",
        "preds_root = repo/\"generated_summary_AmaSum\"\n",
        "test_ids_file = preds_root/\"test_file_names.txt\"\n",
        "\n",
        "# preparing folders\n",
        "gold_dir = repo/\"work_amasum\"/\"gold_test\"\n",
        "inp_dir  = repo/\"work_amasum\"/\"input_texts_test\"\n",
        "os.makedirs(gold_dir, exist_ok=True)\n",
        "os.makedirs(inp_dir, exist_ok=True)\n",
        "\n",
        "ids = [x.strip() for x in open(test_ids_file, \"r\").read().splitlines() if x.strip()]\n",
        "\n",
        "src_test = amasum/\"test\"\n",
        "files = {p.stem: p for p in src_test.glob(\"*.json\")}\n",
        "\n",
        "missing = []\n",
        "for _id in ids:\n",
        "    p = files.get(_id)\n",
        "    if not p:\n",
        "        missing.append(_id)\n",
        "        continue\n",
        "    data = json.loads(p.read_text())\n",
        "\n",
        "    # gold, as per paper\n",
        "    ws = data[\"website_summaries\"][0]\n",
        "    verdict = ws.get(\"verdict\",\"\").strip()\n",
        "    pros   = \". \".join(ws.get(\"pros\",[])).strip()\n",
        "    cons   = \". \".join(ws.get(\"cons\",[])).strip()\n",
        "    gold = \" \".join([x for x in [verdict, pros + (\"\" if pros.endswith(\".\") else \".\" if pros else \"\"), cons + (\"\" if cons.endswith(\".\") else \".\" if cons else \"\")] if x]).strip()\n",
        "    (gold_dir/f\"{_id}.txt\").write_text(gold)\n",
        "\n",
        "    reviews = data.get(\"reviews\", [])\n",
        "    if not reviews:\n",
        "        pos = data.get(\"positive\",\"\")\n",
        "        neg = data.get(\"negative\",\"\")\n",
        "        joined = (pos + \"\\n\" + neg).strip()\n",
        "    else:\n",
        "        joined = \"\\n\".join([r.get(\"text\",\"\") if isinstance(r,dict) else str(r) for r in reviews])\n",
        "    (inp_dir/f\"{_id}.txt\").write_text(joined)\n",
        "\n",
        "print(\"missing\", len(missing))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e2XN-cYDtJY",
        "outputId": "abd5abca-9e55-4980-896e-e1d3804616fb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missing 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, csv, pathlib\n",
        "from evaluate import load\n",
        "\n",
        "root = pathlib.Path(\"/content\")\n",
        "repo = root / \"polarity_calibration_naacl_2024\"\n",
        "preds_root = repo / \"generated_summary_AmaSum\"\n",
        "test_ids_file = preds_root / \"test_file_names.txt\"\n",
        "\n",
        "ids = [line.strip() for line in open(test_ids_file, encoding=\"utf-8\") if line.strip()]\n",
        "\n",
        "systems = {\n",
        "    \"base\": preds_root / \"base_summarizer_flan_t5_large.txt\",\n",
        "    \"poca\": preds_root / \"calibrated_summarizer_PoCa.txt\",\n",
        "}\n",
        "\n",
        "gold_dir = repo / \"work_amasum\" / \"gold_test\"\n",
        "gold_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "rows = []\n",
        "for name, pred_file in systems.items():\n",
        "    hyps_all = [line.strip() for line in open(pred_file, encoding=\"utf-8\")]\n",
        "    n = min(len(ids), len(hyps_all))\n",
        "    refs = [(gold_dir / f\"{ids[i]}.txt\").read_text(encoding=\"utf-8\").strip() for i in range(n)]\n",
        "    hyps = [hyps_all[i] for i in range(n)]\n",
        "    scores = rouge.compute(predictions=hyps, references=refs, use_stemmer=True)\n",
        "    row = {\"system\": name, **{k: round(float(v) * 100, 2) for k, v in scores.items()}}\n",
        "    print(row)\n",
        "    rows.append(row)\n",
        "\n",
        "art = repo / \"artifacts\"\n",
        "art.mkdir(exist_ok=True)\n",
        "with open(art / \"rouge_summary_table.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
        "    w.writeheader()\n",
        "    w.writerows(rows)\n",
        "\n",
        "print(\"saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJtiTuFeE9pM",
        "outputId": "5e5af88b-4ee6-4110-879b-0262e3721bb8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'system': 'base', 'rouge1': 29.22, 'rouge2': 5.64, 'rougeL': 17.19, 'rougeLsum': 17.19}\n",
            "{'system': 'poca', 'rouge1': 28.41, 'rouge2': 5.12, 'rougeL': 16.96, 'rougeLsum': 16.97}\n",
            "saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, numpy as np, torch, csv\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "root = pathlib.Path(\"/content\")\n",
        "repo = root/\"polarity_calibration_naacl_2024\"\n",
        "preds_root = repo/\"generated_summary_AmaSum\"\n",
        "test_ids_file = preds_root/\"test_file_names.txt\"\n",
        "ids = [x.strip() for x in open(test_ids_file, encoding=\"utf-8\") if x.strip()]\n",
        "\n",
        "gold_dir = repo/\"work_amasum\"/\"gold_test\"\n",
        "inp_dir  = repo/\"work_amasum\"/\"input_texts_test\"\n",
        "\n",
        "systems = {\n",
        "    \"base\": preds_root/\"base_summarizer_flan_t5_large.txt\",\n",
        "    \"poca\": preds_root/\"calibrated_summarizer_PoCa.txt\",\n",
        "}\n",
        "\n",
        "def read_lines(p):\n",
        "    return [ln.strip() for ln in open(p, encoding=\"utf-8\").read().splitlines()]\n",
        "\n",
        "def read_txt(p):\n",
        "    return pathlib.Path(p).read_text(encoding=\"utf-8\").strip()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "ckpt = \"siebert/sentiment-roberta-large-english\"\n",
        "tok = AutoTokenizer.from_pretrained(ckpt, use_fast=True)\n",
        "mdl = AutoModelForSequenceClassification.from_pretrained(ckpt).to(device).eval()\n",
        "\n",
        "def pos_prob(text):\n",
        "    sents = [s for s in sent_tokenize(text) if s.strip()]\n",
        "    if not sents:\n",
        "        return 0.5\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for s in sents:\n",
        "            enc = tok(s, truncation=True, return_tensors=\"pt\").to(device)\n",
        "            logits = mdl(**enc).logits\n",
        "            p = torch.softmax(logits, dim=-1)[0,1].item()\n",
        "            probs.append(p)\n",
        "    return float(np.mean(probs))\n",
        "\n",
        "def rmse(xs): return float(np.sqrt(np.mean(np.square(xs))))\n",
        "def mae(xs):  return float(np.mean(np.abs(xs)))\n",
        "\n",
        "rows = []\n",
        "for name, pred_file in systems.items():\n",
        "    hyps_all = read_lines(pred_file)\n",
        "    n = min(len(ids), len(hyps_all))\n",
        "    diffs = []\n",
        "    for i in range(n):\n",
        "        _id = ids[i]\n",
        "        x = read_txt(inp_dir/f\"{_id}.txt\")\n",
        "        y = hyps_all[i]\n",
        "        pin  = pos_prob(x)\n",
        "        pout = pos_prob(y)\n",
        "        diffs.append(pout - pin)\n",
        "    R = {\"system\": name, \"RMSE\": round(rmse(diffs), 4), \"MAE\": round(mae(diffs), 4)}\n",
        "    print(R); rows.append(R)\n",
        "\n",
        "art = repo/\"artifacts\"\n",
        "art.mkdir(exist_ok=True)\n",
        "with open(art/\"polarity_summary_table.csv\",\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"system\",\"RMSE\",\"MAE\"])\n",
        "    w.writeheader(); w.writerows(rows)\n",
        "print(\"saved!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQZ5-SWlS3K-",
        "outputId": "94d577fb-72c7-4e89-9f9b-1ebd7c59affd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'system': 'base', 'RMSE': 0.3542, 'MAE': 0.3355}\n",
            "{'system': 'poca', 'RMSE': 0.3254, 'MAE': 0.3067}\n",
            "saved!!\n"
          ]
        }
      ]
    }
  ]
}